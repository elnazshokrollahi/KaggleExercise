{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "melbourne_file_path = \"/Users/elnazshokrollahi/Documents/GitHub/PythonCodes/Kaggle Exercises/melb_data.csv\"\n",
    "melbourne_data = pd.read_csv (melbourne_file_path)\n",
    "\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "y = filtered_melbourne_data.Price\n",
    "\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melbourne_data [melbourne_features]\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "\n",
    "melbourne_model.fit (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.71594577146544"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)\n",
    "\n",
    "mean_absolute_error (y, predicted_home_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260837.3214977405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split (X,y, random_state =0)\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function min in module builtins:\n",
      "\n",
      "min(...)\n",
      "    min(iterable, *[, default=obj, key=func]) -> value\n",
      "    min(arg1, arg2, *args, *[, key=func]) -> value\n",
      "    \n",
      "    With a single iterable argument, return its smallest item. The\n",
      "    default keyword-only argument specifies an object to return if\n",
      "    the provided iterable is empty.\n",
      "    With two or more arguments, return the smallest argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "#data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "data = melbourne_data\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators = 10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "354257.66157608695\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in X_train.columns\n",
    "                    if X_train[col].isnull().any()]\n",
    "\n",
    "reduced_X_train = X_train.drop (cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop (cols_with_missing, axis=1)\n",
    "\n",
    "print (\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print (score_dataset(reduced_X_train, reduced_X_valid, y_train,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "203078.71828804348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "202839.1169021739\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus [col + '_was_missing'] = X_train_plus [col].isnull()\n",
    "    X_valid_plus [col + '_was_missing'] = X_valid_plus [col].isnull()\n",
    "    \n",
    "    \n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14716, 13)\n",
      "Distance            1\n",
      "Postcode            1\n",
      "Bedroom2         2779\n",
      "Bathroom         2780\n",
      "Car              2860\n",
      "Landsize         3829\n",
      "BuildingArea     8516\n",
      "YearBuilt        7528\n",
      "Lattitude        2675\n",
      "Longtitude       2675\n",
      "Propertycount       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SimpleImputer in module sklearn.impute._base:\n",
      "\n",
      "class SimpleImputer(_BaseImputer)\n",
      " |  SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
      " |  \n",
      " |  Imputation transformer for completing missing values.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <impute>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |     `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n",
      " |     estimator which is now removed.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  missing_values : number, string, np.nan (default) or None\n",
      " |      The placeholder for the missing values. All occurrences of\n",
      " |      `missing_values` will be imputed. For pandas' dataframes with\n",
      " |      nullable integer dtypes with missing values, `missing_values`\n",
      " |      should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n",
      " |  \n",
      " |  strategy : string, default='mean'\n",
      " |      The imputation strategy.\n",
      " |  \n",
      " |      - If \"mean\", then replace missing values using the mean along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"median\", then replace missing values using the median along\n",
      " |        each column. Can only be used with numeric data.\n",
      " |      - If \"most_frequent\", then replace missing using the most frequent\n",
      " |        value along each column. Can be used with strings or numeric data.\n",
      " |      - If \"constant\", then replace missing values with fill_value. Can be\n",
      " |        used with strings or numeric data.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |         strategy=\"constant\" for fixed value imputation.\n",
      " |  \n",
      " |  fill_value : string or numerical value, default=None\n",
      " |      When strategy == \"constant\", fill_value is used to replace all\n",
      " |      occurrences of missing_values.\n",
      " |      If left to the default, fill_value will be 0 when imputing numerical\n",
      " |      data and \"missing_value\" for strings or object data types.\n",
      " |  \n",
      " |  verbose : integer, default=0\n",
      " |      Controls the verbosity of the imputer.\n",
      " |  \n",
      " |  copy : boolean, default=True\n",
      " |      If True, a copy of X will be created. If False, imputation will\n",
      " |      be done in-place whenever possible. Note that, in the following cases,\n",
      " |      a new copy will always be made, even if `copy=False`:\n",
      " |  \n",
      " |      - If X is not an array of floating values;\n",
      " |      - If X is encoded as a CSR matrix;\n",
      " |      - If add_indicator=True.\n",
      " |  \n",
      " |  add_indicator : boolean, default=False\n",
      " |      If True, a :class:`MissingIndicator` transform will stack onto output\n",
      " |      of the imputer's transform. This allows a predictive estimator\n",
      " |      to account for missingness despite imputation. If a feature has no\n",
      " |      missing values at fit/train time, the feature won't appear on\n",
      " |      the missing indicator even if there are missing values at\n",
      " |      transform/test time.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  statistics_ : array of shape (n_features,)\n",
      " |      The imputation fill value for each feature.\n",
      " |      Computing statistics can result in `np.nan` values.\n",
      " |      During :meth:`transform`, features corresponding to `np.nan`\n",
      " |      statistics will be discarded.\n",
      " |  \n",
      " |  indicator_ : :class:`sklearn.impute.MissingIndicator`\n",
      " |      Indicator used to add binary indicators for missing values.\n",
      " |      ``None`` if add_indicator is False.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  IterativeImputer : Multivariate imputation of missing values.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.impute import SimpleImputer\n",
      " |  >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      " |  >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
      " |  SimpleImputer()\n",
      " |  >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
      " |  >>> print(imp_mean.transform(X))\n",
      " |  [[ 7.   2.   3. ]\n",
      " |   [ 4.   3.5  6. ]\n",
      " |   [10.   3.5  9. ]]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Columns which only contained missing values at :meth:`fit` are discarded\n",
      " |  upon :meth:`transform` if strategy is not \"constant\".\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SimpleImputer\n",
      " |      _BaseImputer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the imputer on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Input data, where ``n_samples`` is the number of samples and\n",
      " |          ``n_features`` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : SimpleImputer\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Impute all missing values in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data to complete.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
